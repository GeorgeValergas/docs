# Using Rubber to deploy a Rails app on AWS

This documents contains instructions for using Rubber (v2.7.1) to prepare an app with a
basic framework for a single server deployment on AWS.

## Prepare the app

If you don't have a "secrets" folder already in your app, create a folder to hold secret files
that you don't want checked into source control.

    mkdir -p config/secrets
    
Add Rubber your Gemfile and update your gems with "`bundle install`"

    gem 'rubber', '2.7.1'

If you are pre-compiling assets on the server (the default config), you also need to setup a JavaScript
runtime on the server. The easiest way to do this is adding therubyracer gem to your Gemfile

    group :production do
      gem 'therubyracer', :require => false   # javascript runtime for pre-compiling assets on the server
    end

To avoid installing therubyracer and other production gems on your development machine, run
bundler using the `--without production` option. This is a remembered option, meaning it only
needs to be used once and will be applied automatically after that.

    bundle --without production

## Vulcanize the app to setup the necessary files for Rubber.

We vulcanize each role manually, rather than use an all-in-one generator like "`complete_passenger_mysql`"

We're using munin instead of collecd+graphite because support for collectd+graphite in a Passenger + Nginx
configuration is still in the early stages. We minimize the number of munin charts to minimize the impact
that munin has on the server, so munin is an adequate solution for now. 

    bundle exec rubber vulcanize minimal_passenger_nginx
    bundle exec rubber vulcanize mysql
    bundle exec rubber vulcanize munin
    bundle exec rubber vulcanize monit

## Create config/secrets/rubber-secret.yml

Values entered into `rubber-secret.yml` override corresponding values in `rubber.yml`. Put your AWS
access credentials and other things that you don't want checked into version control in this
file. The contents of this file should look like:

    admin_email: your@email.com

    cloud_providers:
      aws:
        access_key: XXX
        secret_access_key: YYY
        account: Your AWS account ID (digits only, no dashes)
        key_name: your-keypair
        key_file: "#{File.join(RUBBER_ROOT, 'config', 'secrets', cloud_providers.aws.key_name)}"

You can get your Access Credentials and AccountID (`access_key`, `secret_access_key`, `account`) from your
[AWS Account - Security Credentials](https://portal.aws.amazon.com/gp/aws/securityCredentials) page

The `key_name` parameter is the name of the key pair to use for the instance. Key pairs are generated by AWS on demand
and are used to authenticate ssh logins to the instance once it's created. Think of it as a very long password for the instance.
You can create one key pair and share it among all your instances or create a different key pair for each instance. Read more about
these key pairs on the [About AWS Security Credentials](http://docs.amazonwebservices.com/AWSSecurityCredentials/1.0/AboutAWSCredentials.html#EC2KeyPairs) page.

Note that you need to use an EC2 key pair, which can be generated in the [AWS Management Console](https://console.aws.amazon.com/ec2/home?region=us-east-1&#s=KeyPairs)
This is different from a CloudFront key pair and the X.509 Certificates

AWS only gives you the private key portion of the key pair, and only gives you this file once, just after you create it.
It's up to you to save the key file after that. You need to remove the ".pem" extension from the key file and place it in
the config/secrets folder in your app. You then need to create a public key from the private key using the following command:

    ssh-keygen -y -f config/secrets/your-keypair > config/secrets/your-keypair.pub

## Edit rubber.yml to customize your server configuration

Enter all REQUIRED parameters (except AWS access credentials which you defined above) Below are more
details on some of the required parameters.

### Chose your instance type

Tell Rubber what type of instance to create (`image_type`) and what AMI to use as a basis for the instance
(`image_id`). You can find the latest official Ubuntu AMIs here: <http://alestic.com/>
It's always best to check alestic.com for the latest AMIs because they change often. I recommend using Ubuntu
12.04 LTS Precise EBS boot.

    image_type: m1.small
    image_id: ami-de0d9eb7    # Check alestic.com for latest AMI (look in right-hand column for UBUNTU AMIS)

### Setup web tools username and password

Setup a username and password to control access to the web tools (munin/monit). Uncomment the following
lines and add your own username and password.

    web_tools_user: admin
    web_tools_password: yourpw

### Disable auto security groups

By default, Rubber creates security group for all possible server roles in case they are needed in the future.
(AWS doesn't allow you to assign new security groups after you create an instance.) This is not needed in our
case so set `auto_security_groups` to false

    auto_security_groups: false

### Tell Rubber to use rubber-secret.yml

Uncomment the following line and point it to your copy of `rubber-secret.yml`

    rubber_secret: "#{File.join(RUBBER_ROOT, 'config', 'secrets', 'rubber-secret.yml')}"

### Define your hosts and EBS volumes

The host configuration tells rubber which instances are included in your deployment. For our purposes, we are
setting up a single instance.

    hosts:
      cms1:
        instance_roles: "web,app,web_tools,db:primary=true"
        availability_zone: us-east-1b
        use_static_ip: true     # If you want this instance to have a static IP
        volumes:                # (Optional: define one or more EBS volumes)
          - size: 100           # size of vol in GBs
            zone: us-east-1b    # zone to create volume in, needs to match host's zone
            device: /dev/sdh    # OS device to attach volume to
            mount: /ebs         # The directory to mount this volume to
            filesystem: ext3    # the filesystem to create on volume

Under each instance you can define one or more EBS volumes to be created and mounted on the instance.
It's up to you what volumes and mount points to use and what assets to store on the mount points. Rubber
gives you lot of flexibility where assets are places on your server via the various config files.
There is only one **IMPORTANT CAVEAT**: the mount point `/mnt` is already taken by the secondary instance
storage. If you try to use this mount point, Rubber will will create an EBS volume and attach it to your
instance, but silently fail to mount your EBS volume when it discovers that `/mnt` is already taken.

### Update the default mount point in the Rubber recipes and config files

By default Rubber sets up the app, db, log files, ect in /mnt. But this doesn't work if we want to use
and EBS volume for those (as described above). One simple solution is to mount an EBS volume on `/ebs` and
move all assets to `/ebs` by doing a global search and replace in your app, replacing `"/mnt/"` with `"/ebs/"`

## Make a few config changes

### Edit rubber-passenger_nginx.yml

Change the HTTP ports. Rubber assumes haproxy is always installed and sets up the HTTP ports accordingly.
Since we are not using haproxy, we need to tell passenger to listen on ports 80,443

    passenger_listen_port: 80
    passenger_listen_ssl_port: 443

Reduce the number of workers. 20 is too high for a small server. I try to set the number of workers
so that there is at least 1 "spare" worker that isn't processing many requests. Running `passenger-status`
on the server after it's been up a few days will tell you how many requests each worker is processing.

    max_app_connections: 5

### Edit config/rubber/common/database.yml.

Add a socket line and comment out the host line. The socket connection is faster and will work on a
single-server deployment.

    socket: /var/run/mysqld/mysqld.sock
    # (socket is faster on single-server deploys) host: <%= rubber_instances.for_role('db', 'primary' => true).first.full_name %>

Hard-code the adapter. The dynamic logic Rubber fails because we don't push database.yml into our repo.
Remove this line

    adapter: <%= YAML::load(File.open("#{Rubber.root}/config/database.yml"))["production"]["adapter"] %>

Add this line:

    adapter: mysql2

### Edit config/rubber/role/passenger_nginx/nginx.conf

Add `passenger_max_requests` to restart worker processes after x requests. This is the easiest way
I can find to keep memory bloat under control.

    passenger_max_requests 4000;

## Edit deploy.rb to customize the deploy process

### Enable push\_instance\_config

When `push_instance_config` is enabled, Rubber pushes `instance-production.yml` directly to the server during
deploy, overriding the version checked into git. If we didn't use this, we would need to check in changes to
`instance-production.yml` and push them to github after each deploy step.

    set :push_instance_config, true

### Add a task to push the contents of config/secrets to the server

`config/secrets` contains files with sensitive information that should not be checked into source control.
We need to push these files to the server manually during deploy.

    namespace :deploy do
      # Push contents of config/secrets folder to the server
      after "deploy:update_code", "deploy:app_secrets"
      desc "Push contents of config/secrets folder to the remote server"
      task :app_secrets do
        run "mkdir -p #{release_path}/config/secrets"
        transfer(:up, "config/secrets", "#{release_path}/config/secrets") { print "." }
      end
    end
    
### Use github for deploy

The trick here is to use `ssh_options[:forward_agent] = true` so that the server can use our
local github key to pull the deploy. Also use remote_cache to speed up deploys.

    # Based on http://github.com/guides/deploying-with-capistrano
    default_run_options[:pty] = true
    set :scm, :git
    set :repository, "git@github.com:username/repo.git"
    set :branch, "the_branch"
    ssh_options[:forward_agent] = true  # Magic! lets the server use our local github key to pull the deploy
    set :deploy_via, :remote_cache
    
## Backups via EBS snapshots (optional)

EBS snapshots provide an excellent backup mechanism for applications running on AWS.
The task below is hardcoded to work with a single-server deployment with a single EBS
volume and MySQL database. To use EBS snapshots, create a rake file lib/tasks/system.rake

    require "fog"

    desc "Backup server and clean old backups"
    task :backup => ['backup:run', 'backup:clean']

    namespace :backup do

      desc "Backup server - database and all user data will be backed up"
      task :run => :environment do
        start = Time.now
        # Flush and lock all the DB tables. Rails will block on actions that write to the DB
        # until the tables are unlocked. This should be transparent to web users, asside from
        # a short delay in the app response time. Entire :backup task only takes a few seconds.
        ActiveRecord::Base.establish_connection
        ActiveRecord::Base.connection.execute("FLUSH TABLES WITH READ LOCK")
        # Fush Ext3 file system cache to disk
        system("sync")
        # Create EBS snapshot. We only have one instance and one EBS volume, just select that volume
        fog = Fog::Compute.new(:provider => 'AWS', :aws_access_key_id => Rubber.config.cloud_providers.aws.access_key, :aws_secret_access_key => Rubber.config.cloud_providers.aws.secret_access_key)
        volume_id = Rubber.instances.first.volumes.first
        puts "Creating snapshot of #{volume_id}."
        snap = fog.snapshots.new :volume_id => volume_id, :description => "Nightly backup of #{Rubber.instances.first.name}"
        snap.save
        # unlock tables
        ActiveRecord::Base.connection.execute("UNLOCK TABLES")
        puts "System backup completed in %.1f seconds." % [Time.now - start]
      end

      desc "Clean up old snapshots - keep daily snapshots for 1 month, keep monthly snapshots indefinitely"
      task :clean => :environment do
        start = Time.now
        fog = Fog::Compute.new(:provider => 'AWS', :aws_access_key_id => Rubber.config.cloud_providers.aws.access_key, :aws_secret_access_key => Rubber.config.cloud_providers.aws.secret_access_key)
        volume_id = Rubber.instances.first.volumes.first
        fog.snapshots.all.each do |snapshot|
          if snapshot.volume_id==volume_id || snapshot.volume_id=='vol-e6b5fc9d'  # (hack the PSIApps volume in here for now - vol-e6b5fc9d)
            if snapshot.created_at < 31.days.ago && snapshot.created_at.day!=1
              puts "DELETING #{snapshot.id} (#{snapshot.created_at.strftime('%b %-d, %Y')}) for #{snapshot.volume_id} (#{snapshot.volume_size}mb)"
              fog.delete_snapshot(snapshot.id)
            else
              puts "Keeping #{snapshot.id} (#{snapshot.created_at.strftime('%b %-d, %Y')}) for #{snapshot.volume_id} (#{snapshot.volume_size}mb)"
            end
          end
        end
        puts "Clean backups completed in %.1f seconds." % [Time.now - start]
      end
    end
    
Add a cron job to `config/rubber/common/crontab` to run system:backup nightly

    # Backup server at 1:30am
    30 1 * * * <%= Rubber.root %>/script/rubber cron --rake backup

Disable the Rubber-provided backup cron job defined in `config/rubber/role/db/crontab`

    #(disabled) 0 */3 * * * <%= Rubber.root %>/script/rubber cron --task util:backup_db ...

NOTE: EBS snapshot backups are required for the last two recovery methods described below.

## Commit and push all config files to the repo

Before creating the server, you must commit the rubber/config files to the repo and push to the branch
you're deploying from. Even though the rubber/config files are being pushed directly to the server during
deploy, the config/rubber directory must exist for this to work.

## Create the server and deploy the application.

    bundle exec cap rubber:create        # creates the instance on AWS
    bundle exec cap rubber:bootstrap     # sets up the instance, installs all require packages, ruby, etc
    bundle exec cap deploy:cold          # deploy the app (use first time only)

During the rubber:create step you will be asked for two pieces of information:

* **Instance alias**: This is the alias of the server you want to create. In our case, we defined a single
alias (cms1) so enter that here. It's possible to define several hosts in `rubber.yml` and create
them all at once here.

* **Instance roles**: Just press enter to accept the defaults you configured in `instance_rolls` in `rubber.yml`

**NOTE: Check the console output carefully for errors after each step.** You don't need to scan the entire
console output, just the last few lines. Rubber is good about stopping if there is an error and will display
a meaningful error message. But with so much console output you might not notice unless you pay attention.
Error missed during bootstrap and deploy can lead to strange problems that are very difficult to diagnose later on.

If your application has db seeds, SSH into server (see instructions below) and seed the database

    current                     # RUN THIS ON THE SERVER (change to the current deployment folder)
    bundle exec rake db:seed    # RUN THIS ON THE SERVER

On subsequent deploys, you only need to deploy and optionally migrate the database

    bundle exec cap deploy               # deploy the app (after the first deploy)
    bundle exec cap deploy:migrate       # (if needed) to install db migrations

You should be able to re-run rubber:bootstrap any time. If the bootstrapping process is interrupted, 
rubber:bootstrap is smart enough to pick up where it left off. If, for some reason, rubber:bootstrap
is unable to do this and is returning unexplained errors, in particular "Instance not found for host:
ip-10-2-118-252", try running rubber:refresh

    bundle exec cap rubber:refresh      # Refresh hostname aliases on the server

## Connect to the new instance via SSH

    ssh -i config/secrets/your-keypair-name root@your.server.url

## Cleanup Munin Charts

If left in the default configuration, Munin charts take about 20s to render every 5 minutes, maxing out the CPU. 
To reduce the number of charts Munin generates, SSH into the instance and run these commands. The charts
selected below seem to provide a good balance with minimal load.

    cd /etc/munin/plugins
    rm *
    ln -s /usr/share/munin/plugins/cpu cpu
    ln -s /usr/share/munin/plugins/df df
    ln -s /usr/share/munin/plugins/http_loadtime http_loadtime
    ln -s /usr/share/munin/plugins/load load
    ln -s /usr/share/munin/plugins/memory memory
    ln -s /usr/share/munin/plugins/munin_stats munin_stats

You will also need to remove several files from you app. Otherwise several charts will be recreated on
the next deploy.

    rm script/munin/example_mysql_query.rb
    rm script/munin/example_simple.rb
    rm config/rubber/role/passenger_nginx/munin-passenger-memory.conf
    rm config/rubber/role/passenger_nginx/munin-passenger.conf

## Swap File

The Ubuntu AMIs don't use swap files. To setup a swap file on the server, follow instructions here
<http://serverfault.com/questions/218750/why-ec2-ubuntu-images-dont-have-swap>

#Using Rubber to deploy an app on AWS
This documents contains instructions for using Rubber (v2.10) to prepare an app with a basic framework
for a single server deployment on AWS.

##Prepare the app
If you don't have a "secrets" folder already in your app, create a folder to hold secret files
that you don't want checked into source control.

    mkdir -p config/secrets
    
Add the following lines to your Gemfile and update your gems with "`bundle install`"

    gem 'rubber'

##Vulcanize the app to setup the necessary files for Rubber.
We vulcanize each role manually, rather than use an all-in-one generator like "`complete_passenger_mysql`"

We're using munin instead of collecd+graphite because Rubber doesn't yet support collectd+graphite in a
Passenger + Nginx configuration. We minimize the number of munin charts to minimize the impact that munin
has on the server, so munin is an adequate solution for now. 

    bundle exec rubber vulcanize minimal_passenger_nginx
    bundle exec rubber vulcanize mysql
    bundle exec rubber vulcanize monit
    bundle exec rubber vulcanize munin

##Create config/secrets/rubber-secret.yml
Values entered into `rubber-secret.yml` override corresponding values in `rubber.yml`. Put your AWS
access credentials and other things that you don't want checked into version control in this
file. The contents of this file should look like:

    admin_email: your@email.com

    cloud_providers:
      aws:
        access_key: XXX
        secret_access_key: YYY
        account: Your AWS account ID (digits only, no dashes)
        key_name: your-keypair
        key_file: "#{File.join(RUBBER_ROOT, 'config', 'secrets', cloud_providers.aws.key_name)}"

You can get your Access Credentials and AccountID (`access_key`, `secret_access_key`, `account`) from your
[AWS Account - Security Credentials](https://portal.aws.amazon.com/gp/aws/securityCredentials) page

The `key_name` parameter is the name of the key pair to use for the instance. Key pairs are generated by AWS on demand
and are used to authenticate ssh logins to the instance once it's created. Think of it as a very long password for the instance.
You can create one key pair and share it among all your instances or create a different key pair for each instance. Read more about
these key pairs on the [About AWS Security Credentials](http://docs.amazonwebservices.com/AWSSecurityCredentials/1.0/AboutAWSCredentials.html#EC2KeyPairs) page.

Note that you need to use an EC2 key pair, which can be generated in the [AWS Management Console](https://console.aws.amazon.com/ec2/home?region=us-east-1&#s=KeyPairs)
This is different from a CloudFront key pair and the X.509 Certificates

AWS only gives you the private key portion of the key pair, and only gives you this file once, just after you create it.
It's up to you to save the key file after that. You need to remove the ".pem" extension from the key file and place it in
the config/secrets folder in your app. You then need to create a public key from the private key using the following command:

    ssh-keygen -y -f config/secrets/your-keypair > config/secrets/your-keypair.pub

##Edit rubber.yml to customize your server configuration
Enter all REQUIRED parameters (except AWS access credentials which you defined above) Below are more
details on some of the required parameters.

###Chose your instance type
Tell Rubber what type of instance to create (`image_type`) and what AMI to use as a basis for the instance
(`image_id`). You can find the latest official Ubuntu AMIs here: <http://alestic.com/>
It's always best to check alestic.com for the latest AMIs because they change often. I recommend using Ubuntu
12.04 LTS Precise EBS boot.

    image_type: m1.small
    image_id: ami-013f9768    # Check alestic.com for latest AMI (look in right-hand column for UBUNTU AMIS)

###Setup web tools username and password
Setup a username and password to control access to the web tools (munin/monit). Uncomment the following
lines and add your own username and password.

    web_tools_user: admin
    web_tools_password: yourpw

###Disable auto security groups
By default, Rubber creates security group for all possible server roles in case they are needed in the future.
(AWS doesn't allow you to assign new security groups after you create an instance.) This is not needed in our
case so set `auto_security_groups` to false

    auto_security_groups: false

###Tell Rubber to use rubber-secret.yml
Uncomment the following line and point it to your copy of `rubber-secret.yml`

    rubber_secret: "#{File.join(RUBBER_ROOT, 'config', 'secrets', 'rubber-secret.yml')}"

###Define your hosts and EBS volumes
The host configuration tells rubber which instances are included in your deployment. For our purposes, we are
setting up a single instance.

    hosts:
      cms1:
        instance_roles: "web,app,web_tools,db:primary=true"
        availability_zone: us-east-1b
        use_static_ip: true     # If you want this instance to have a static IP
        volumes:                # (Optional: define one or more EBS volumes)
          - size: 100           # size of vol in GBs
            zone: us-east-1b    # zone to create volume in, needs to match host's zone
            device: /dev/sdh    # OS device to attach volume to
            mount: /ebs         # The directory to mount this volume to
            filesystem: ext3    # the filesystem to create on volume

Under each instance you can define one or more EBS volumes to be created and mounted on the instance.
It's up to you what volumes and mount points to use and what assets to store on the mount points. Rubber
gives you lot of flexibility where assets are places on your server via the various config files.
There is only one **IMPORTANT CAVEAT**: the mount point `/mnt` is already taken by the secondary instance
storage. If you try to use this mount point, Rubber will will create an EBS volume and attach it to your
instance, but silently fail to mount your EBS volume when it discovers that `/mnt` is already taken.

###Update the default mount point in the Rubber recipes and config files
By default Rubber sets up the app, db, log files, ect in /mnt. But this doesn't work if we want to use
and EBS volume for those (as described above). One simple solution is to mount an EBS volume on `/ebs` and
move all assets to `/ebs` by doing a global search and replace in your app, replacing `"/mnt/"` with `"/ebs/"`

##Make a few config changes

###Edit rubber-passenger_nginx.yml and change the HTTP ports
Rubber assumes haproxy is always installed and sets up the HTTP ports accordingly. Since we are not using haproxy
at this time, we need to tell passenger to listen on ports 80,443

    passenger_listen_port: 80
    passenger_listen_ssl_port: 443

###Edit config/rubber/common/database.yml.
Add a socket line and comment out the host line. The socket connection is faster and will work on a
single-server deployment.

    socket: /var/run/mysqld/mysqld.sock
    # (socket is faster on single-server deploys) host: <%= rubber_instances.for_role('db', 'primary' => true).first.full_name %>

Hard-code the adapter. The dynamic logic Rubber fails because we don't push database.yml into our repo.
Remove this line

    adapter: <%= YAML::load(File.open("#{Rubber.root}/config/database.yml"))["production"]["adapter"] %>

Add this line:

    adapter: mysql2

###Edit deploy-mysql.rb (will be fixed in next Rubber release)
MySQL 5.5, which is included with Unbuntu 12.04, has an Anonymous Account that conflicts with the account
that Rubber creates. We must delete this account during the deploy. Add the following line in the
`create_master_db` script

    rubber.sudo_script "create_master_db", <<-ENDSCRIPT
      mysql -u root -e "create database #{env.db_name};"
      mysql -u root -e "delete from mysql.user where user='' and host='localhost';"    <<-- ADD THIS LINE
      ...
    ENDSCRIPT

###Edit passenger_nginx/application.conf (pull request pending)
Add the following block to the bottom of the file to optimize caching of precompiled assets

    # Give static assets a far-future header and serve the pre-compressed version of the asset
    # instead of compressing on the fly.
    location ~ ^/(assets)/ {
      gzip_static on;
      expires     max;
      add_header  Cache-Control public;
      add_header  Last-Modified "";
      add_header  ETag "";
      break;
    }

##Edit deploy.rb to customize the deploy process
###Enable push\_instance\_config
When `push_instance_config` is enabled, Rubber pushes `instance-production.yml` directly to the server during
deploy, overriding the version checked into git. If we didn't use this, we would need to check in changes to
`instance-production.yml` and push them to github after each deploy step.

    set :push_instance_config, true

###Add a task to push the contents of config/secrets to the server
`config/secrets` contains files with sensitive information that should not be checked into source control.
We need to push these files to the server manually during deploy.

    namespace :deploy do
      # Push contents of config/secrets folder to the server
      after "deploy:update_code", "deploy:app_secrets"
      desc "Push contents of config/secrets folder to the remote server"
      task :app_secrets do
        transfer(:up, "config/secrets", "#{release_path}/config/secrets") { print "." }
      end
    end
    
###Add a task to precompile assets
This task compiles assets on the dev system and then pushes them up to the server. This avoids
several problems caused by compiling assets on the server, including blowing the CPU burst window
on t1.micro instances and tricky timing issues related to when the assets need to be compiled in
the deploy cycle (after rubber:update, but before server restart).

    namespace :deploy do
      desc "precompile and deploy the assets to the server"
      after "deploy:update_code", "deploy:precompile_assets"
      task :precompile_assets, :roles => :app do
        run_locally "#{rake} RAILS_ENV=#{rails_env} RAILS_GROUPS=assets assets:precompile"
        transfer(:up, "public/assets", "#{release_path}/public/assets") { print "." }
        run_locally "rm -rf public/assets"    # clean up to avoid conflicts with development-mode assets
      end
    end

###Remove the Rubber deploy assets task
This is not longer needed because we are precompiling assets locally. Delete these lines

    if Rubber::Util.has_asset_pipeline?
      # load asset pipeline tasks, and reorder them to run after
      ...
    end

###Use github for deploy
The trick here is to use `ssh_options[:forward_agent] = true` so that the server can use our
local github key to pull the deploy. Also use remote_cache to speed up deploys.

    # Based on http://github.com/guides/deploying-with-capistrano
    default_run_options[:pty] = true
    set :scm, :git
    set :repository, "git@github.com:username/repo.git"
    set :branch, "the_branch"
    ssh_options[:forward_agent] = true  # Magic! lets the server use our local github key to pull the deploy
    set :deploy_via, :remote_cache
    
##Backups via EBS snapshots (optional)
EBS snapshots provide an excellent backup mechanism for applications running on AWS.
The task below is hardcoded to work with a single-server deployment with a single EBS
volume and MySQL database. To use EBS snapshots, create a rake file lib/tasks/system.rake

    require "fog"

    desc "Backup server and clean old backups"
    task :backup => ['backup:run', 'backup:clean']

    namespace :backup do

      desc "Backup server - database and all user data will be backed up"
      task :run => :environment do
        start = Time.now
        # Flush and lock all the DB tables. Rails will block on actions that write to the DB
        # until the tables are unlocked. This should be transparent to web users, asside from
        # a short delay in the app response time. Entire :backup task only takes a few seconds.
        ActiveRecord::Base.establish_connection
        ActiveRecord::Base.connection.execute("FLUSH TABLES WITH READ LOCK")
        # Fush Ext3 file system cache to disk
        system("sync")
        # Create EBS snapshot. We only have one instance and one EBS volume, just select that volume
        fog = Fog::Compute.new(:provider => 'AWS', :aws_access_key_id => Rubber.config.cloud_providers.aws.access_key, :aws_secret_access_key => Rubber.config.cloud_providers.aws.secret_access_key)
        volume_id = Rubber.instances.first.volumes.first
        puts "Creating snapshot of #{volume_id}."
        snap = fog.snapshots.new :volume_id => volume_id, :description => "Nightly backup of #{Rubber.instances.first.name}"
        snap.save
        # unlock tables
        ActiveRecord::Base.connection.execute("UNLOCK TABLES")
        puts "System backup completed in %.1f seconds." % [Time.now - start]
      end

      desc "Clean up old snapshots - keep daily snapshots for 1 month, keep monthly snapshots indefinitely"
      task :clean => :environment do
        start = Time.now
        fog = Fog::Compute.new(:provider => 'AWS', :aws_access_key_id => Rubber.config.cloud_providers.aws.access_key, :aws_secret_access_key => Rubber.config.cloud_providers.aws.secret_access_key)
        volume_id = Rubber.instances.first.volumes.first
        fog.snapshots.all.each do |snapshot|
          if snapshot.volume_id==volume_id || snapshot.volume_id=='vol-e6b5fc9d'  # (hack the PSIApps volume in here for now - vol-e6b5fc9d)
            if snapshot.created_at < 31.days.ago && snapshot.created_at.day!=1
              puts "DELETING #{snapshot.id} (#{snapshot.created_at.strftime('%b %-d, %Y')}) for #{snapshot.volume_id} (#{snapshot.volume_size}mb)"
              fog.delete_snapshot(snapshot.id)
            else
              puts "Keeping #{snapshot.id} (#{snapshot.created_at.strftime('%b %-d, %Y')}) for #{snapshot.volume_id} (#{snapshot.volume_size}mb)"
            end
          end
        end
        puts "Clean backups completed in %.1f seconds." % [Time.now - start]
      end
    end
    
Add a cron job to `config/rubber/common/crontab` to run system:backup nightly

    # Backup server at 1:30am
    30 1 * * * <%= Rubber.root %>/script/rubber cron --rake backup

Disable the Rubber-provided backup cron job defined in `config/rubber/role/db/crontab`

    #(disabled) 0 */3 * * * <%= Rubber.root %>/script/rubber cron --task util:backup_db ...

NOTE: EBS snapshot backups are required for the last two recovery methods described below.

## Commit and push all config files to the repo
Before creating the server, you must commit the rubber/config files to the repo and push to the branch
you're deploying from. Even though the rubber/config files are being pushed directly to the server during
deploy, the config/rubber directory must exist for this to work.

##Create the server and deploy the application.

    bundle exec cap rubber:create        # creates the instance on AWS
    bundle exec cap rubber:bootstrap     # sets up the instance, installs all require packages, ruby, etc
    bundle exec cap deploy:cold          # deploy the app (use first time only)

During the rubber:create step you will be asked for two pieces of information:

* **Instance alias**: This is the alias of the server you want to create. In our case, we defined a single
alias (cms1) so enter that here. It's possible to define several hosts in `rubber.yml` and create
them all at once here.

* **Instance roles**: Just press enter to accept the defaults you configured in `instance_rolls` in `rubber.yml`

The entire creation and deployment process takes about 25 minutes with the bulk of the time in the bootstrap step.

**NOTE: Check the console output carefully for errors after each step.** You don't need to scan the entire
console output, just the last few lines. Rubber is good about stopping if there is an error and will display
a meaningful error message. But with so much console output you might not notice unless you pay attention.
Error missed during bootstrap and deploy can lead to strange problems that are very difficult to diagnose later on.

If your application has db seeds, SSH into server (see instructions below) and seed the database

    current                     # RUN THIS ON THE SERVER (change to the current deployment folder)
    bundle exec rake db:seed    # RUN THIS ON THE SERVER

On subsequent deploys, you only need to deploy and optionally migrate the database

    bundle exec cap deploy               # deploy the app (afte the first deploy)
    bundle exec cap deploy:migrate       # (if needed) to install db migrations

Run at any time to update packages on the server.

    bundle exec cap rubber:bootstrap     # updates packages on the server if server is already bootstrapped

You should be able to re-run rubber:bootstrap any time. If the bootstrapping process is interrupted, 
rubber:bootstrap is smart enough to pick up where it left off. If, for some reason, rubber:bootstrap
is unable to do this and is returning unexplained errors, in particular "Instance not found for host:
ip-10-2-118-252", try running rubber:refresh

    bundle exec cap rubber:refresh      # Refresh hostname aliases on the server

##Connect to the new instance via SSH

    ssh -i config/secrets/your-keypair-name root@your.server.url

##Cleanup Munin Charts
If left in the default configuration, Munin charts take about 20s to render every 5 minutes, maxing out the CPU. 
To reduce the number of charts Munin generates, SSH into the instance and run these commands. The charts
selected below seem to provide a good balance with minimal load.

    cd /etc/munin/plugins
    rm *
    ln -s /usr/share/munin/plugins/cpu cpu
    ln -s /usr/share/munin/plugins/diskstats diskstats
    ln -s /usr/share/munin/plugins/http_loadtime http_loadtime
    ln -s /usr/share/munin/plugins/load load
    ln -s /usr/share/munin/plugins/memory memory
    ln -s /usr/share/munin/plugins/munin_stats munin_stats
    ln -s /usr/share/munin/plugins/swap swap

You will also need to remove several files from you app. Otherwise several charts will be recreated on
the next deploy.

    rm script/munin/example_mysql_query.rb
    rm script/munin/example_simple.rb
    rm config/rubber/role/passenger_nginx/munin-passenger-memory.conf
    rm config/rubber/role/passenger_nginx/munin-passenger.conf

##Other Cleanup
Configure the popularity contest package. This prevents failures in /etc/cron.daily/popularity-contest
The packages is installed but not configured. It's a known problem with the 12.04 ami image:
<https://bugs.launchpad.net/ubuntu/+source/popularity-contest/+bug/707311> To configure it
run this command on the server

    sudo dpkg-reconfigure popularity-contest

Create upstart log directory. This prevents failures in /etc/cron.daily/logrotate. It's a known problem
with the 12.04 ami image: <http://osdir.com/ml/ubuntu-bugs/2012-04/msg63377.html>

    mkdir /var/log/upstart


# Recovering from failures

##Recovering if an instance crashes
If an instance crashes, Rubber allows you to create a new instance and remount your EBS volumes on the new
instance. All assets on the EBS volumes will be preserved. The static IP associated with the instance will
also be preserved and reused when you recreate the instance.

Tell Rubber the instance has crashed by destroying it.

    bundle exec cap rubber:destroy
    
IMPORTANT: Make sure to say "No" when asked "Instance has persistent volumes, do you want to destroy them? [y/N]?:"    
    
Recreate and redeploy the a new instance

    bundle exec cap rubber:create         # creates the instance on AWS
    bundle exec cap rubber:bootstrap      # sets up the instance, installs all require packages, ruby, etc
    bundle exec cap rubber:mysql:restart  # Restart MySQL so it looks in the right place for the data folder (why is this needed?)
    bundle exec cap deploy                # deploy the app

Rubber will attach and mount the existing EBS volumes during the bootstrap. Rubber will not format or overwrite
data on an existing EBS volume.

##Recovering if the EBS volume is corrupted or both the instance and the EBS volume are lost
This is a little heavy-handed if the instance has not been lost. But it's simpler than recreating and
attaching an EBS volume to a running instance.

Shut down the instance (or tell Rubber the instance has crashed) by destroying it.

    bundle exec cap rubber:destroy

Say "No" when asked "Instance has persistent volumes, do you want to destroy them? [y/N]?:"
This leaves the volume setting in instance-production.yml which we can use later.

Using AWS web-based console

* Delete the corrupted EBS volume
* Create a new EBS volume from the most recent nightly snapshot. Make a note of the new volume ID

Edit instance-production.yml and put the new volume ID under the "volumes:" section

    - volumes: 
        cwa2_/dev/sdh: vol-46de282c     <<-- NEW VOLUME ID GOES HERE
      static_ips: 
        cwa2: 50.16.189.180

Create a new instance using the steps in "Recovering if an instance crashes"
